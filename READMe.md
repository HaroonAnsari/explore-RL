# explore-RL
This repository serves as an essential guide to the field of Reinforcement Learning (RL), offering a curated list of both foundational and recent influential papers. It is divided into two main sections: the first contains the seminal papers that have shaped the foundation of RL, and the second provides a chronological list of key papers that reflect the latest advancements and trends in the field. Whether you're a student, a researcher, or an enthusiast, this collection is designed to keep you up-to-date and deepen your understanding of the ever-evolving landscape of RL.

## Foundational Papers

### Classical papers
* **\[1989\] Learning to predict by the methods of temporal differences** [\[PDF\]](http://incompleteideas.net/papers/sutton-88-with-erratum.pdf)
* **\[1992\] Q-learning** [\[PDF\]](https://www.gatsby.ucl.ac.uk/~dayan/papers/cjch.pdf)
* **\[1995\] Temporal Difference Learning and TD-Gammon** [\[PDF\]](https://www.csd.uwo.ca/~xling/cs346a/extra/tdgammon.pdf)
* **\[2000\] Policy Gradient Methods for Reinforcement Learning with Function Approximation** [\[PDF\]](https://homes.cs.washington.edu/~todorov/courses/amath579/reading/PolicyGradient.pdf)
* **\[2002\] Policy invariance under reward transformations: Theory and application to reward shaping** [\[PDF\]](http://ai.stanford.edu/~ang/papers/shaping-icml99.pdf)

### 2013
* **Playing Atari with Deep Reinforcement Learning** [\[PDF\]](https://arxiv.org/pdf/1312.5602.pdf)

### 2014
* **Deterministic Policy Gradient** [\[PDF\]](http://proceedings.mlr.press/v32/silver14.pdf)

### 2015
* **Human-level control through deep reinforcement learning (DQN)** [\[PDF\]](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)
* **Prioritized Experience Replay** [\[PDF\]](https://arxiv.org/pdf/1511.05952.pdf)
* **Deep Reinforcement Learning with Double Q-learning (Double-DQN)** [\[PDF\]](https://arxiv.org/pdf/1509.06461.pdf)
* **Trust Region Policy Optimization (TRPO)** [\[PDF\]](https://arxiv.org/pdf/1502.05477.pdf)
* **High-Dimensional Continuous Control Using Generalized Advantage Estimation (GAE)** [\[PDF\]](https://arxiv.org/pdf/1506.02438.pdf)
* **Continuous control with deep reinforcement learning (DDPG)** [\[PDF\]](https://arxiv.org/pdf/1509.02971.pdf)
* **Universal Value Function Approximators** [\[PDF\]](http://proceedings.mlr.press/v37/schaul15.pdf)

## Recent Papers

### 2016
* **Asynchronous Methods for Deep Reinforcement Learning** [\[PDF\]](https://arxiv.org/pdf/1602.01783.pdf)
* **Sample Efficient Actor-Critic with Experience Replay** [\[PDF\]](https://arxiv.org/pdf/1611.01224.pdf)
* **Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic** [\[PDF\]](https://arxiv.org/pdf/1611.02247.pdf)
* **Combining policy gradient and Q-learning** [\[PDF\]](https://arxiv.org/pdf/1611.01626.pdf)
* **Progressive Neural Networks** [\[PDF\]](https://arxiv.org/pdf/1606.04671.pdf)

### 2017
* **Proximal Policy Optimization Algorithms** [\[PDF\]](https://arxiv.org/pdf/1707.06347.pdf)
* **A Distributional Perspective on Reinforcement Learning** [\[PDF\]](https://arxiv.org/pdf/1707.06887.pdf)
* **The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning** [\[PDF\]](https://arxiv.org/pdf/1704.04651.pdf)
* **Curiosity-driven Exploration by Self-supervised Prediction** [\[PDF\]](https://arxiv.org/pdf/1705.05363.pdf)
* **FeUdal Networks for Hierarchical Reinforcement Learning** [\[PDF\]](https://arxiv.org/pdf/1703.01161.pdf)
* **Imagination-Augmented Agents for Deep Reinforcement Learning** [\[PDF\]](https://arxiv.org/pdf/1707.06203.pdf)
* **Deep reinforcement learning from human preferences** [\[PDF\]](https://arxiv.org/pdf/1706.03741.pdf)

### 2018
* **Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor** [\[PDF\]](https://arxiv.org/pdf/1801.01290.pdf)
* **Addressing Function Approximation Error in Actor-Critic Methods**[\[PDF\]](https://arxiv.org/pdf/1802.09477.pdf)

### 2019
* **Model Based Reinforcement Learning for Atari**[\[PDF\]](https://arxiv.org/pdf/1903.00374v3.pdf)
* **Dota 2 with Large Scale Deep Reinforcement Learning**[\[PDF\]](https://arxiv.org/pdf/1912.06680v1.pdf)

### 2020
* **Revisiting Fundamentals of Experience Replay**[\[PDF\]](https://arxiv.org/pdf/2007.06700.pdf)

### 2021
* **Reward is enough**[\[PDF\]](https://pdf.sciencedirectassets.com/271585/1-s2.0-S0004370221X00070/1-s2.0-S0004370221000862/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEOn%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJGMEQCIDD1mkTrzRgPaZFChi76qBVxpA%2F1oEfvgeNaKHsngYNQAiAR%2FvP%2B04fl9lISGsVesVLQDOpdOiNWneOgD8hnm8BSZyq8BQih%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAUaDDA1OTAwMzU0Njg2NSIMJ1IlZaTCw%2FYHVxKhKpAFLm93ArWg2SLucYTMCbXRvbqLgmDfCi6G2t4yGNzAFP28EEh6VQ5R%2FoTQcfCPy5io0xI0OlxARTwCCr479y4VRYqfA2QugkQCP5XJGvjj92SBs0iqVofmavFOI7%2FfWjbFrmYF4oQlo7FAX9f51Q7AScmmvAD4MyYNs%2FBSnSZlvl%2FSww%2Bz4u5AAuE1MuektMVmIkXtwrc3uR0HZ9lPIgkTFQtrWD17C1%2FXzeaYwXCG%2Bx0O%2Bpz4RkehShexSKM5HvirxwkV3IxyxxRkxx1RL99%2BCbcyHGFL%2BsUwGSjFU4L6%2FxwjHFYK%2B1BE%2Fm11fnTMMseAeYNDtsPCxfqxMGwKpRIfW7AJtb%2Fplp%2BBajEBqzV3zk%2BL9FwrgucF1lUY2mj9RMOfP0eQXuW6KNuvO1uN5JRL4%2BxOy5kY0IZb1CkwQ8T%2Fg6AFn7AnNlfoKmWvc6qWMnCdYMtziTti6xFbZ2iO16nhNoqu%2FsaoqX2Vis1XMtVaNyGQ9KWG5h8TqsS4kM93wW9OIx5Rg1cBKSyno3zkuloIw%2FguALjLv%2FQsZPfBfZmvvhTm9R4LlOGpSvz28L0RAHmY2IqlwiYvk7wBDmzpF1K2iIyFbdNIxWe7MNUfkEHlaVV1I6dQQDzWgD6U2azHmzj6S8maCa6hpJD3zaBuAZo2liptiNFuVAtQ7q7uD2bYjJyJS%2BecgG7Liil%2BvOjaV%2BZaVAluYQJX%2FZ1GUNW682hVDIH48m4gJkrwx8HKHxwDRo6mgnq0lHDv3QPUXENv8DkJrtAz59KI%2F4oQN2rpvGWbty8XHgvWa3L9myEDMyIFDuwNFFj%2FylkFpDe%2FqaHe5rervshhHf4N6gGd6aqoIOmuvDY7xM9t2nWke03dj%2FE0r%2B4wlvaBpwY6sgFFcygdEMU1Irc3FsjKeQrL0HYpNTPpTuTh4gdje8tBgSA24nHd1iWLYG0w3yWhE7uzhrbApxCIochzsxXsiQASXfVpzDiocv5r4aiqj1ZNa3Few4pii%2BpoTDLXTmFkFj9PurE4OlG0osXkVvVep7S5yLVCa5pIbsilqp6f9oygo9ue%2BoeVw5log1atPkdbGD9IveuvlMsTrBaYbTz7pgP40R1eqfjBVLRHRSwbgTRhK370&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20230819T093928Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYREOQ4MOR%2F20230819%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=95c7c09809b2908c931b65298badb8ebf8af380abc0647ea072ac78374b2ec6f&hash=111c703d077d3e3441d4c5836abc52ec7457c0c53ec05ca32d159e4544b0eb86&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0004370221000862&tid=spdf-0ac74b55-5383-47de-8cea-dead81ef0adc&sid=9fbf1fc688bcd34f2059980161b3c4dd1a3agxrqb&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=130f55065b525101055b&rr=7f916df4eb9e9361&cc=in)
* **Deep Reinforcement Learning at the Edge of the Statistical Precipice**[\[PDF\]](https://openreview.net/pdf?id=uqv8-U4lKBe)
* **Open-Ended Learning Leads to Generally Capable Agents**[\[PDF\]](https://storage.googleapis.com/deepmind-media/papers/Open-Ended%20Learning%20Leads%20to%20Generally%20Capable%20Agents/open-ended-learning-paper.pdf)
* **Decision Transformer: Reinforcement Learning via Sequence Modeling**[\[PDF\]](https://arxiv.org/pdf/2106.01345.pdf)
